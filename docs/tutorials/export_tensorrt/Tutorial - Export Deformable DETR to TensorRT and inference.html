

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Deformable DETR &mdash; Aloception 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Aloscene" href="../../aloscene/aloscene.html" />
    <link rel="prev" title="DETR" href="Tutorial%20-%20Export%20DETR%20to%20TensorRT%20and%20inference.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Aloception
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Geting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/aloscene.html">Aloscene: Computer vision with ease</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/alodataset.html">Alodataset: Loading your vision datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/alonet.html">Alonet : Training your models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/augmented_tensor.html">About augmented tensors</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Turorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../data_setup.html">How to setup your data?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training_detr.html">Training Detr</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finetuning_detr.html">Finetuning DETR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training_panoptic.html">Training Panoptic Head module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training_deformable_detr.html">Training Deformable DETR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finetuning_deformable_detr.html">Finetuning Deformanble DETR</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../tensort_inference.html">Exporting DETR / Deformable-DETR to TensorRT</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Tutorial%20-%20Export%20DETR%20to%20TensorRT%20and%20inference.html">DETR</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Deformable DETR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Load-from-.pth-checkpoint">Load from .pth checkpoint</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Inference-with-TensorRT">Inference with TensorRT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Load-weight-from-run_id">Load weight from run_id</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Aloception API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../aloscene/aloscene.html">Aloscene</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../alodataset/alodataset.html">Alodataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../alonet/alonet.html">Alonet</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Aloception</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../tensort_inference.html">Exporting DETR / Deformable-DETR to TensorRT</a> &raquo;</li>
        
      <li>Deformable DETR</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/tutorials/export_tensorrt/Tutorial - Export Deformable DETR to TensorRT and inference.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">alonet</span>
<span class="kn">from</span> <span class="nn">alonet.common</span> <span class="kn">import</span> <span class="n">pl_helpers</span>
<span class="c1"># for Deformable DETR</span>
<span class="kn">from</span> <span class="nn">alonet.deformable_detr</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DeformableDETR</span><span class="p">,</span>
    <span class="n">DeformableDetrR50Refinement</span><span class="p">,</span>
    <span class="n">DeformableDetrR50RefinementFinetune</span><span class="p">,</span>
    <span class="n">LitDeformableDetr</span>
<span class="p">)</span>
<span class="c1"># for TensorRT</span>
<span class="kn">from</span> <span class="nn">alonet.deformable_detr.trt_exporter</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DeformableDetrTRTExporter</span><span class="p">,</span>
    <span class="n">load_trt_plugins_for_deformable_detr</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">alonet.torch2trt</span> <span class="kn">import</span> <span class="n">TRTExecutor</span>

<span class="kn">import</span> <span class="nn">aloscene</span>
<span class="kn">from</span> <span class="nn">aloscene</span> <span class="kn">import</span> <span class="n">Frame</span>

<span class="c1"># Deformable DETR requires custom TensorRT plugin for Multiscale Deformable Attention</span>
<span class="n">load_trt_plugins_for_deformable_detr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>In this tutorial, we will convert Deformable DETR in TensorRT in order to reduce memory footprint and inference time.</p>
<p>The model weight can be loaded either from a .pth file or from a run_id using <code class="docutils literal notranslate"><span class="pre">Aloception</span></code> API.</p>
<p>This notebook might crash if there is not enough GPU memory. In this case you can reduce the image size or run only cells from either <strong>Load from .pth checkpoint</strong> or <strong>Inference with TensorRT</strong> or <strong>Load weight from run_id</strong>.</p>
<div class="line-block">
<div class="line">The workflow is:</div>
<div class="line">1.Load model and trained weights</div>
<div class="line">2.Instantiate corresponding TensorRT exporter</div>
<div class="line">3.Run the export</div>
<div class="line">4.Grab a cup of coffee or take some air while waiting :)</div>
</div>
<p>Now, let’s define some constant what we will use throughout this tutorial.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">INPUT_SHAPE</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1280</span><span class="p">,</span> <span class="mi">1920</span><span class="p">]</span> <span class="c1"># [C, H, W], change input shape if needed</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">PRECISION</span> <span class="o">=</span> <span class="s2">&quot;fp16&quot;</span> <span class="c1"># or &quot;fp32&quot;</span>
</pre></div>
</div>
</div>
<p>The input dimension is [B, C, H, W] of which [C, H, W] is defined by <code class="docutils literal notranslate"><span class="pre">INPUT_SHAPE</span></code> and B is defined by <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">PRECISION</span></code> defines the precision of model weights. It is either “fp32” or “fp16” for float32 and float16 respectively.</p>
<p>We will run inference in a test image for qualitative comparison between PyTorch and TensorRT</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">image_path</span> <span class="o">=</span> <span class="s2">&quot;PATH/TO/IMAGE&quot;</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Frame</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">INPUT_SHAPE</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">norm_resnet</span><span class="p">()</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">Frame</span><span class="o">.</span><span class="n">batch_list</span><span class="p">([</span><span class="n">frame</span><span class="p">])</span>
<span class="n">img</span><span class="o">.</span><span class="n">get_view</span><span class="p">()</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="Deformable-DETR">
<h1>Deformable DETR<a class="headerlink" href="#Deformable-DETR" title="Permalink to this headline">¶</a></h1>
<div class="section" id="Load-from-.pth-checkpoint">
<h2>Load from .pth checkpoint<a class="headerlink" href="#Load-from-.pth-checkpoint" title="Permalink to this headline">¶</a></h2>
<p>In this example, we use weight Deformable DETR R50 with iterative box refinement trained on COCO from <a class="reference external" href="https://github.com/fundamentalvision/Deformable-DETR">official repository Deformable DETR</a> but the workflow is valid for any finetuned model with its associated .pth file.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 1. Instantiate model and load trained weight</span>
<span class="n">weight_path</span> <span class="o">=</span> <span class="s2">&quot;PATH/TO/CHECKPOINT.pth&quot;</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="n">background_class</span> <span class="o">=</span> <span class="mi">91</span> <span class="c1"># COCO classes</span>

<span class="n">torch_model</span> <span class="o">=</span> <span class="n">DeformableDetrR50Refinement</span><span class="p">(</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
    <span class="n">aux_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># we don&#39;t want auxilary outputs</span>
<span class="p">)</span>
<span class="n">torch_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">alonet</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">torch_model</span><span class="p">,</span> <span class="n">weight_path</span><span class="p">,</span> <span class="n">torch_model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 2. Instantiate corresponding exporter</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">weight_path</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># Because the exporter will use ONNX format as an intermediate bridge</span>
<span class="c1"># between PyTorch and TensorRT, we need to specify a path where the ONNX file will be save.</span>
<span class="n">onnx_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">weight_path</span><span class="p">),</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s2">&quot;.onnx&quot;</span><span class="p">)</span>

<span class="n">exporter</span> <span class="o">=</span> <span class="n">DeformableDetrTRTExporter</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">torch_model</span><span class="p">,</span>
    <span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_path</span><span class="p">,</span>
    <span class="n">input_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">INPUT_SHAPE</span><span class="p">,),</span>
    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;img&quot;</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">precision</span><span class="o">=</span><span class="n">PRECISION</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">torch_model</span><span class="o">.</span><span class="n">device</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 3. Run the exporter</span>
<span class="n">exporter</span><span class="o">.</span><span class="n">export_engine</span><span class="p">()</span>
<span class="n">engine_path</span> <span class="o">=</span> <span class="n">exporter</span><span class="o">.</span><span class="n">engine_path</span>
</pre></div>
</div>
</div>
<p>After the export, 2 files will be created in the root directory containing the checkpoint file.</p>
<div class="line-block">
<div class="line">ROOT_DIR</div>
<div class="line">|__ MODEL.pth</div>
<div class="line">|__ MODEL.onnx</div>
<div class="line">|__ MODEL_PRECISION.engine</div>
</div>
<p>The .onnx file is a ONNX graph which serves as intermediate bridge between PyTorch and TensorRT. The .engine file is the model serialized as TensorRT engine. For deployment and inference, .engine file will be deserialized and executed by TensorRT.</p>
<div class="section" id="Inference-with-TensorRT">
<h3>Inference with TensorRT<a class="headerlink" href="#Inference-with-TensorRT" title="Permalink to this headline">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">DeformableDetrInference</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="n">background_class</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">load_trt_plugins_for_deformable_detr</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">background_class</span> <span class="o">=</span> <span class="n">background_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">activation_fn</span>

    <span class="k">def</span> <span class="nf">get_outs_filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DeformableDETR</span><span class="o">.</span><span class="n">get_outs_filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward_out</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">forward_out</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">forward_out</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">forward_out</span><span class="p">}</span>
        <span class="n">forward_out</span><span class="p">[</span><span class="s2">&quot;activation_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span>
        <span class="k">return</span> <span class="n">DeformableDETR</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward_out</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In other to benefit the inference logic implemented in alonet Deformable DETR without instantiating the whole model in PyTorch, we create a helper class which calls <code class="docutils literal notranslate"><span class="pre">DeformableDETR.inference</span></code> method.</p>
<p>In alonet we implemented the classification head with either sigmoid activation or softmax activation. In order to use the <code class="docutils literal notranslate"><span class="pre">DeformableDETR.inference</span></code> correctly, we need to define <code class="docutils literal notranslate"><span class="pre">activation_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">background_class</span></code> in case of softmax function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">trt_model</span> <span class="o">=</span> <span class="n">TRTExecutor</span><span class="p">(</span><span class="n">engine_path</span><span class="p">)</span>
<span class="n">trt_model</span><span class="o">.</span><span class="n">print_bindings_info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>The input <code class="docutils literal notranslate"><span class="pre">img</span></code> shape is (B, C, H, W) with C=4 because we concatenate RGB image (B, 3, H, W) and its mask of shape (B, 1, H, W) containing 1 on padded pixels.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">m_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">frame</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(),</span> <span class="n">frame</span><span class="o">.</span><span class="n">mask</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">()],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">trt_m_outputs</span> <span class="o">=</span> <span class="n">trt_model</span><span class="p">(</span><span class="n">m_input</span><span class="p">)</span>
<span class="n">trt_pred_boxes</span> <span class="o">=</span> <span class="n">DeformableDetrInference</span><span class="p">()(</span><span class="n">trt_m_outputs</span><span class="p">)</span>

<span class="c1"># visualize the result</span>
<span class="n">trt_pred_boxes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_view</span><span class="p">(</span><span class="n">frame</span><span class="o">=</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># compare with the result from model in PyTorch</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">torch_m_outputs</span> <span class="o">=</span> <span class="n">torch_model</span><span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch_model</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
<span class="n">torch_pred_boxes</span> <span class="o">=</span> <span class="n">torch_model</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">torch_m_outputs</span><span class="p">)</span>
<span class="n">torch_pred_boxes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_view</span><span class="p">(</span><span class="n">frame</span><span class="o">=</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>A quick qualitative comparison show that 2 models give nearly identical results. The minor difference is from the fact that we use the precision float16 for our TensorRT engine which is not the case for the model in PyTorch.</p>
</div>
</div>
<div class="section" id="Load-weight-from-run_id">
<h2>Load weight from run_id<a class="headerlink" href="#Load-weight-from-run_id" title="Permalink to this headline">¶</a></h2>
<p>After having trained your DETR model using aloception API, we can load the model from a run_id and export it to TensorRT using the same workflow.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Define the train project and the run_id from which we want to load weight</span>
<span class="n">project</span> <span class="o">=</span> <span class="s2">&quot;YOUR_PROJECT_NAME&quot;</span>
<span class="n">run_id</span> <span class="o">=</span> <span class="s2">&quot;YOUR_RUN_ID&quot;</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;MODEL_NAME&quot;</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># number of classes in your finetune model</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 1. Instantiate the model and load weight from run_id</span>
<span class="n">torch_model</span> <span class="o">=</span> <span class="n">DeformableDetrR50RefinementFinetune</span><span class="p">(</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="c1"># only person class</span>
    <span class="n">aux_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># we don&#39;t want auxilary outputs</span>
<span class="p">)</span>

<span class="n">lit_model</span> <span class="o">=</span> <span class="n">pl_helpers</span><span class="o">.</span><span class="n">load_training</span><span class="p">(</span>
    <span class="n">LitDeformableDetr</span><span class="p">,</span> <span class="c1"># The PyTorch Lightning Module that was used in training</span>
    <span class="n">project_run_id</span><span class="o">=</span><span class="n">project</span><span class="p">,</span>
    <span class="n">run_id</span><span class="o">=</span><span class="n">run_id</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">torch_model</span>
<span class="p">)</span>
<span class="n">torch_model</span> <span class="o">=</span> <span class="n">lit_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 2. Instantiate the exporter</span>
<span class="c1"># Because the exporter will use ONNX format as an intermediate bridge</span>
<span class="c1"># between PyTorch and TensorRT, we need to specify a path where the ONNX file will be save.</span>
<span class="n">project_dir</span><span class="p">,</span> <span class="n">run_id_dir</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pl_helpers</span><span class="o">.</span><span class="n">get_expe_infos</span><span class="p">(</span><span class="n">project</span><span class="p">,</span> <span class="n">run_id</span><span class="p">)</span>
<span class="n">onnx_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">run_id_dir</span><span class="p">,</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s2">&quot;.onnx&quot;</span><span class="p">)</span>

<span class="n">exporter</span> <span class="o">=</span> <span class="n">DeformableDetrTRTExporter</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">torch_model</span><span class="p">,</span>
    <span class="n">onnx_path</span><span class="o">=</span><span class="n">onnx_path</span><span class="p">,</span>
    <span class="n">input_shapes</span><span class="o">=</span><span class="p">(</span><span class="n">INPUT_SHAPE</span><span class="p">,),</span>
    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;img&quot;</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">precision</span><span class="o">=</span><span class="n">PRECISION</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">torch_model</span><span class="o">.</span><span class="n">device</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># 3. Run the exporter</span>
<span class="n">exporter</span><span class="o">.</span><span class="n">export_engine</span><span class="p">()</span>
<span class="n">engine_path</span> <span class="o">=</span> <span class="n">exporter</span><span class="o">.</span><span class="n">engine_path</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Test inference</span>
<span class="n">trt_model</span> <span class="o">=</span> <span class="n">TRTExecutor</span><span class="p">(</span><span class="n">engine_path</span><span class="p">)</span>
<span class="n">trt_model</span><span class="o">.</span><span class="n">print_bindings_info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">DeformableDetrInference</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="n">background_class</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">load_trt_plugins_for_deformable_detr</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">background_class</span> <span class="o">=</span> <span class="n">background_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">activation_fn</span>

    <span class="k">def</span> <span class="nf">get_outs_filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DeformableDETR</span><span class="o">.</span><span class="n">get_outs_filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward_out</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">forward_out</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">forward_out</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">forward_out</span><span class="p">}</span>
        <span class="n">forward_out</span><span class="p">[</span><span class="s2">&quot;activation_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span>
        <span class="k">return</span> <span class="n">DeformableDETR</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward_out</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Test inference</span>
<span class="n">m_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">frame</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(),</span> <span class="n">frame</span><span class="o">.</span><span class="n">mask</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">()],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">trt_m_outputs</span> <span class="o">=</span> <span class="n">trt_model</span><span class="p">(</span><span class="n">m_input</span><span class="p">)</span>
<span class="c1"># In DeformableDetrInference, use `activation_fn` and `background_class` if needed</span>
<span class="n">trt_pred_boxes</span> <span class="o">=</span> <span class="n">DeformableDetrInference</span><span class="p">()(</span><span class="n">trt_m_outputs</span><span class="p">)</span>

<span class="c1"># visualize the result</span>
<span class="n">trt_pred_boxes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_view</span><span class="p">(</span><span class="n">frame</span><span class="o">=</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># compare with the result from model in PyTorch</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">torch_m_outputs</span> <span class="o">=</span> <span class="n">torch_model</span><span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch_model</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
<span class="n">torch_pred_boxes</span> <span class="o">=</span> <span class="n">torch_model</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">torch_m_outputs</span><span class="p">)</span>
<span class="n">torch_pred_boxes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_view</span><span class="p">(</span><span class="n">frame</span><span class="o">=</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>As explained above, the comparison show that 2 models give nearly identical results. The difference is from the fact that we use the precision float16 for our TensorRT engine.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../../aloscene/aloscene.html" class="btn btn-neutral float-right" title="Aloscene" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="Tutorial%20-%20Export%20DETR%20to%20TensorRT%20and%20inference.html" class="btn btn-neutral float-left" title="DETR" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Visual Behavior.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>